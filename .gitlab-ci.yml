image: gitlab.green-rabbit.net:5050/kimata/local-ubuntu:250706_043ea01d

variables:
    UV_LINK_MODE: copy
    UV_CACHE_DIR: .uv-cache

stages:
    - generate-tag
    - build
    - test
    - tag-latest
    - deploy
    - renovate

generate-tag:
    stage: generate-tag
    script:
        - echo "TAG=$(date +%y%m%d)_${CI_COMMIT_SHORT_SHA}" > tag.env
    artifacts:
        reports:
            dotenv: tag.env
        expire_in: 1 hour

build-react:
    stage: build

    needs: []

    image: node:24.1

    script:
        - cd react
        - npm ci --cache .npm --prefer-offline
        - npm run build

    artifacts:
        paths:
            - react/dist/
    cache:
        key: "${CI_JOB_NAME}"
        paths:
            - react/.npm/
            - react/node_modules/

build-image:
    stage: build

    needs:
        - generate-tag
        - build-react

    variables:
        BUILDER: builder

    before_script:
        - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY

    script:
        - 'echo "Building: ${CI_REGISTRY_IMAGE}:${TAG}"'

        - git clone https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.green-rabbit.net/kimata/hems-config.git
        - mv hems-config/unit-cooler.yaml config.yaml

        - |
            docker buildx create \
                --name ${BUILDER} \
                --driver-opt image=${BUILD_KIT_IMAGE} \
                --use \
                --config /etc/buildkitd.toml

        - docker buildx use ${BUILDER}
        - docker buildx inspect --bootstrap
        - >
            docker buildx build --provenance=false --progress=plain --platform linux/amd64,linux/arm64/v8
            --cache-from type=registry,ref=${CI_REGISTRY_IMAGE}:cache
            --cache-from type=registry,ref=${CI_REGISTRY_IMAGE}:latest
            --cache-to type=inline --cache-to type=registry,ref=${CI_REGISTRY_IMAGE}:cache,mode=max
            --build-arg IMAGE_BUILD_DATE=$(date --iso-8601=seconds)
            --tag ${CI_REGISTRY_IMAGE}:${TAG} --push .

test-prepare:
    stage: build

    script:
        - apt update
        - apt install --no-install-recommends --assume-yes swig

        - uv sync --locked --no-editable

    artifacts:
        paths:
            - ${UV_CACHE_DIR}
        expire_in: 1 hour

    cache:
        - key:
              files:
                  - uv.lock
          paths:
              - ${UV_CACHE_DIR}

.cache-base:
    cache:
        - key:
              files:
                  - uv.lock
          paths:
              - ${UV_CACHE_DIR}

test-walk-through:
    extends: .cache-base
    stage: test
    needs:
        - test-prepare
    script:
        - >
            uv run pytest --numprocesses=auto --junit-xml=tests/evidence/junit-report.xml
            --maxfail=1 --dist=loadgroup tests/test_basic.py tests/test_error_handling.py
    artifacts:
        when: always
        paths:
            - tests/evidence/**
        reports:
            junit: tests/evidence/junit-report.xml

# Base templates
.docker-cleanup-base:
    before_script:
        - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY
        - >
            docker ps --filter "label=job=${CI_JOB_NAME}" | grep hours | cut -d " " -f1 |
            while read id; do docker stop -t 5 $id; done || true

.docker-test-base:
    extends: .docker-cleanup-base
    needs:
        - generate-tag
        - build-image

.test-controller-base:
    extends: .docker-test-base
    stage: test
    script:
        - >
            docker run --rm --tty=true --name ${CI_JOB_NAME}-${CI_JOB_ID}
            --label job=${CI_JOB_NAME} ${CI_REGISTRY_IMAGE}:${TAG}
            ./src/controller.py ${CONTROLLER_ARGS}
test-controller-sample:
    extends: .test-controller-base
    variables:
        CONTROLLER_ARGS: "-c config.example.yaml -N -n 1 -t 10"

test-controller-regular:
    extends: .test-controller-base
    variables:
        CONTROLLER_ARGS: "-N -n 1 -t 10"

.test-actuator-base:
    extends: .docker-test-base
    stage: test

    variables:
        CURL_IMAGE: curlimages/curl:8.1.2

    script:
        - >
            docker run --rm --detach=true --tty=true --name ${CI_JOB_NAME}-${CI_JOB_ID}-1
            --label job=${CI_JOB_NAME} ${CI_REGISTRY_IMAGE}:${TAG} ./src/controller.py -d -t 2

        - >
            CTRL_IP=$(docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'
            ${CI_JOB_NAME}-${CI_JOB_ID}-1)
        - '[ -n "$CTRL_IP" ] || (echo "Failed to get Controller IP" && exit 1)'
        - 'echo "Controller: ${CTRL_IP}"'
        - >
            docker run --rm --detach=true --tty=true --name ${CI_JOB_NAME}-${CI_JOB_ID}-2
            --label job=${CI_JOB_NAME} ${CI_REGISTRY_IMAGE}:${TAG}
            ./src/actuator.py -n 3 -d ${ACTUATOR_ARGS}
            -s $(docker inspect
            --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'
            ${CI_JOB_NAME}-${CI_JOB_ID}-1)
        - sleep 10
        - docker ps -a --filter name=${CI_JOB_NAME}-${CI_JOB_ID}

        # NOTE: Fail した時の原因究明用にこの時点のログを保存する
        - docker logs ${CI_JOB_NAME}-${CI_JOB_ID}-1 > controller_log.txt
        - docker logs ${CI_JOB_NAME}-${CI_JOB_ID}-2 > actuator_log.txt

        - >
            ACT_IP=$(docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'
            ${CI_JOB_NAME}-${CI_JOB_ID}-2)
        - 'echo "Actuator: ${ACT_IP}"'
        # Wait for actuator to be ready
        - URL="http://${ACT_IP}:5001/unit-cooler/api/log_view"
        - CURL_OPTS="--silent --fail --head --connect-timeout 5 --max-time 10"
        - >
            for i in {1..20}; do
                if docker run --rm ${CURL_IMAGE} ${CURL_OPTS} ${URL} > /dev/null 2>&1; then
                    echo "Actuator is ready"
                    break
                fi
                echo "Waiting for actuator to be ready... ($i/20)"
            done

        - >
            docker run --rm ${CURL_IMAGE} --head
            http://$(docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'
            ${CI_JOB_NAME}-${CI_JOB_ID}-2):5001/unit-cooler/api/log_view

        - docker logs ${CI_JOB_NAME}-${CI_JOB_ID}-2 > controller_log.txt
        - docker attach --no-stdin ${CI_JOB_NAME}-${CI_JOB_ID}-2

        - docker logs ${CI_JOB_NAME}-${CI_JOB_ID}-1 > controller_log.txt
        - docker stop ${CI_JOB_NAME}-${CI_JOB_ID}-1

    artifacts:
        when: always
        paths:
            - controller_log.txt
            - actuator_log.txt

test-actuator-sample:
    extends: .test-actuator-base
    variables:
        CONTROL_HOST: 192.168.0.20
        ACTUATOR_ARGS: "-c config.example.yaml"

test-actuator-regular:
    extends: .test-actuator-base
    variables:
        ACTUATOR_ARGS: ""

test-webui-docker:
    extends: [.docker-test-base, .cache-base]
    stage: test
    needs:
        - generate-tag
        - build-image
        - test-prepare
    script:
        - uv run playwright install --with-deps chromium
        - >
            docker run --rm --detach=true --tty=true --name ${CI_JOB_NAME}-${CI_JOB_ID}-1
            --label job=${CI_JOB_NAME} ${CI_REGISTRY_IMAGE}:${TAG} ./src/controller.py -d

        - >
            CTRL_IP=$(docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'
            ${CI_JOB_NAME}-${CI_JOB_ID}-1)
        - '[ -n "$CTRL_IP" ] || (echo "Failed to get Controller IP" && exit 1)'
        - 'echo "Controller: ${CTRL_IP}"'

        - >
            docker run --rm --detach=true --tty=true --name ${CI_JOB_NAME}-${CI_JOB_ID}-2
            --label job=${CI_JOB_NAME} ${CI_REGISTRY_IMAGE}:${TAG}
            ./src/actuator.py -d
            -s $(docker inspect
            --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'
            ${CI_JOB_NAME}-${CI_JOB_ID}-1)

        - >
            ACT_IP=$(docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'
            ${CI_JOB_NAME}-${CI_JOB_ID}-2)
        - '[ -n "$ACT_IP" ] || (echo "Failed to get Actuator IP" && exit 1)'
        - 'echo "Actuator: ${ACT_IP}"'

        - >
            docker run --rm --detach=true --tty=true --name ${CI_JOB_NAME}-${CI_JOB_ID}-3
            --label job=${CI_JOB_NAME} --publish :5000
            ${CI_REGISTRY_IMAGE}:${TAG}
            ./src/webui.py
            -s $(docker inspect
            --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'
            ${CI_JOB_NAME}-${CI_JOB_ID}-1)
            -a $(docker inspect
            --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'
            ${CI_JOB_NAME}-${CI_JOB_ID}-2)

        # NOTE: Fail した時の原因究明用にこの時点のログを保存する
        - docker logs ${CI_JOB_NAME}-${CI_JOB_ID}-1 > controller_log.txt
        - docker logs ${CI_JOB_NAME}-${CI_JOB_ID}-2 > actuator_log.txt
        - docker logs ${CI_JOB_NAME}-${CI_JOB_ID}-3 > webui_log.txt

        - >
            WEB_PORT=$(docker port "${CI_JOB_NAME}-${CI_JOB_ID}-3" 5000 | cut -d: -f2)
        - >
            WEB_HOST=$(docker network inspect bridge --format="{{range .IPAM.Config}}{{.Gateway}}{{end}}")
        - >
            WEB_URL="http://${WEB_HOST}:${WEB_PORT}/unit-cooler/"
        - >
            echo "WEB_URL: ${WEB_URL}"

        - >
            timeout 60 bash -c "until curl --fail --connect-timeout 5 --max-time 10 \"${WEB_URL}\" > /dev/null 2>&1;
            do echo 'Retrying in 5 seconds...'; sleep 5; done" || ACCESS_FAILED=1
        - |
            if [ "$ACCESS_FAILED" = "1" ]; then
                echo "Failed to access WEB app"
                docker logs ${CI_JOB_NAME}-${CI_JOB_ID}-1 > controller_log.txt
                docker logs ${CI_JOB_NAME}-${CI_JOB_ID}-2 > actuator_log.txt
                docker logs ${CI_JOB_NAME}-${CI_JOB_ID}-3 > webui_log.txt
                exit 1
            fi

        - >
            uv run pytest tests/test_playwright.py --host ${WEB_HOST} --port ${WEB_PORT}

        - docker logs ${CI_JOB_NAME}-${CI_JOB_ID}-1 > controller_log.txt
        - docker logs ${CI_JOB_NAME}-${CI_JOB_ID}-2 > actuator_log.txt
        - docker logs ${CI_JOB_NAME}-${CI_JOB_ID}-3 > webui_log.txt
        - docker stop ${CI_JOB_NAME}-${CI_JOB_ID}-1
        - docker stop ${CI_JOB_NAME}-${CI_JOB_ID}-2
        - docker stop ${CI_JOB_NAME}-${CI_JOB_ID}-3
    artifacts:
        when: always
        paths:
            - controller_log.txt
            - actuator_log.txt
            - webui_log.txt
            - tests/evidence/**

test-healthz-docker:
    extends: .docker-test-base
    stage: test

    # 同一ジョブの並行実行を防止してリソース競合を回避
    resource_group: ${CI_JOB_NAME}

    script:
        # クリーンアップ: 古いコンテナを停止
        - >
            docker ps --filter "label=job=${CI_JOB_NAME}" --format "{{.ID}}" |
            xargs -r docker stop -t 10 || true

        # クリーンアップ: 古いネットワークを削除
        - docker network rm ${CI_PROJECT_NAME}-${CI_JOB_NAME}-network 2>/dev/null || true

        # 専用ネットワーク作成（プロジェクト固有）
        - docker network create ${CI_PROJECT_NAME}-${CI_JOB_NAME}-network
        - >
            docker run --rm --detach=true --tty=true
            --name ${CI_PROJECT_NAME}-${CI_JOB_NAME}-CTRL
            --label job=${CI_JOB_NAME}
            --network ${CI_PROJECT_NAME}-${CI_JOB_NAME}-network
            ${CI_REGISTRY_IMAGE}:${TAG} ./src/controller.py

        - >
            CTRL_IP=$(docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'
            ${CI_PROJECT_NAME}-${CI_JOB_NAME}-CTRL)
        - 'echo "Controller: ${CTRL_IP}"'
        - >
            docker run --rm --detach=true --tty=true
            --name ${CI_PROJECT_NAME}-${CI_JOB_NAME}-ACT
            --label job=${CI_JOB_NAME}
            --network ${CI_PROJECT_NAME}-${CI_JOB_NAME}-network
            ${CI_REGISTRY_IMAGE}:${TAG}
            ./src/actuator.py -d
            -s $(docker inspect
            --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'
            ${CI_PROJECT_NAME}-${CI_JOB_NAME}-CTRL)

        - >
            ACT_IP=$(docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'
            ${CI_PROJECT_NAME}-${CI_JOB_NAME}-ACT)
        - '[ -n "$ACT_IP" ] || (echo "Failed to get Actuator IP" && exit 1)'
        - 'echo "Actuator: ${ACT_IP}"'

        - >
            docker run --rm --detach=true --tty=true
            --name ${CI_PROJECT_NAME}-${CI_JOB_NAME}-WEB
            --label job=${CI_JOB_NAME}
            --network ${CI_PROJECT_NAME}-${CI_JOB_NAME}-network
            ${CI_REGISTRY_IMAGE}:${TAG}
            ./src/webui.py
            -s $(docker inspect
            --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'
            ${CI_PROJECT_NAME}-${CI_JOB_NAME}-CTRL)
            -a $(docker inspect
            --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'
            ${CI_PROJECT_NAME}-${CI_JOB_NAME}-ACT)

        - echo "Containers started, checking status..."
        - docker ps | grep ${CI_PROJECT_NAME}-${CI_JOB_NAME}

        # Controllerの初回liveness更新を確認
        - echo "Waiting for controller to start updating liveness..."
        - >
            for i in {1..20}; do
                if docker exec ${CI_PROJECT_NAME}-${CI_JOB_NAME}-CTRL ls /dev/shm/healthz.controller 2>/dev/null; then
                    echo "Controller liveness file created after $i attempts"
                    break
                fi
                echo "Waiting for controller liveness file... ($i/20)"
                sleep 5
            done

        # NOTE: healthz をチェックする前にその時点のログを取得しておく
        - docker logs ${CI_PROJECT_NAME}-${CI_JOB_NAME}-CTRL > controller_log.txt
        - docker logs ${CI_PROJECT_NAME}-${CI_JOB_NAME}-ACT > actuator_log.txt
        - docker logs ${CI_PROJECT_NAME}-${CI_JOB_NAME}-WEB > webui_log.txt

        # ヘルスチェック実行
        - echo "Running health checks..."
        - docker exec ${CI_PROJECT_NAME}-${CI_JOB_NAME}-CTRL ./src/healthz.py -m CTRL
        - docker exec ${CI_PROJECT_NAME}-${CI_JOB_NAME}-ACT ./src/healthz.py -m ACT
        - docker exec ${CI_PROJECT_NAME}-${CI_JOB_NAME}-WEB ./src/healthz.py -m WEB

        # 最終ログ取得
        - docker logs ${CI_PROJECT_NAME}-${CI_JOB_NAME}-CTRL > controller_log.txt
        - docker logs ${CI_PROJECT_NAME}-${CI_JOB_NAME}-ACT > actuator_log.txt
        - docker logs ${CI_PROJECT_NAME}-${CI_JOB_NAME}-WEB > webui_log.txt

        # クリーンアップ
        - docker stop ${CI_PROJECT_NAME}-${CI_JOB_NAME}-CTRL || true
        - docker stop ${CI_PROJECT_NAME}-${CI_JOB_NAME}-ACT || true
        - docker stop ${CI_PROJECT_NAME}-${CI_JOB_NAME}-WEB || true
        - docker network rm ${CI_PROJECT_NAME}-${CI_JOB_NAME}-network || true
    artifacts:
        when: always
        paths:
            - controller_log.txt
            - actuator_log.txt
            - webui_log.txt

tag-latest:
    stage: tag-latest
    needs:
        - generate-tag
        - job: test-controller-regular
          artifacts: false
        - job: test-actuator-regular
          artifacts: false
        - job: test-webui-docker
          artifacts: false
        - job: test-healthz-docker
          artifacts: false
        - job: test-walk-through
          artifacts: false

    before_script:
        - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY

    script:
        - 'echo "Tagging multi-arch image ${CI_REGISTRY_IMAGE}:${TAG} as latest"'
        - docker buildx imagetools create -t ${CI_REGISTRY_IMAGE}:latest ${CI_REGISTRY_IMAGE}:${TAG}

    rules:
        - if: $CI_COMMIT_BRANCH != $CI_DEFAULT_BRANCH
          when: never
        - when: on_success

# Base template for deployment jobs
deploy:
    stage: deploy
    image:
        name: gitlab.green-rabbit.net:5050/kimata/local-kubectl:250715_c88f3965

    needs:
        - generate-tag
        - job: test-controller-regular
          artifacts: false
        - job: test-actuator-regular
          artifacts: false
        - job: test-webui-docker
          artifacts: false
        - job: test-healthz-docker
          artifacts: false
        - job: test-walk-through
          artifacts: false

    script:
        - 'IMAGE="${CI_REGISTRY_IMAGE}:${TAG}"'
        - 'echo "Deploying controller image: $IMAGE"'
        - kubectl config get-contexts
        - kubectl config use-context kimata/outdoor_unit_cooler:pod-rollout

        - kubectl -n hems set image deployment/unit-cooler-controller unit-cooler-controller=${IMAGE}
        - kubectl -n hems set image deployment/unit-cooler-actuator unit-cooler-actuator=${IMAGE}
        - kubectl -n hems set image deployment/unit-cooler-webui unit-cooler-webui=${IMAGE}
        - kubectl -n hems set image deployment/unit-cooler-webui-demo unit-cooler-webui-demo=${IMAGE}

        - kubectl -n hems rollout status deployment/unit-cooler-controller --timeout=300s
        - kubectl -n hems rollout status deployment/unit-cooler-actuator --timeout=300s
        - kubectl -n hems rollout status deployment/unit-cooler-webui --timeout=300s
        - kubectl -n hems rollout status deployment/unit-cooler-webui-demo --timeout=300s

    rules:
        # NOTE: 自動実行ではデプロイしない
        - if: $CI_PIPELINE_SOURCE == "schedule"
          when: never
        - if: $CI_COMMIT_BRANCH != $CI_DEFAULT_BRANCH
          when: never
        - when: on_success

renovate:
    stage: renovate
    image:
        name: gitlab.green-rabbit.net:5050/kimata/local-renovate:250715_3b8866ff

    script:
        - renovate --platform gitlab --token ${RENOVATE_TOKEN} --endpoint ${CI_SERVER_URL}/api/v4 ${CI_PROJECT_PATH}
        # # NOTE: ついでにお掃除
        #  - docker rm $(docker ps -a --filter "status=exited" -q) || true
    rules:
        - if: '$CI_COMMIT_BRANCH != "master"'
          when: never
        - if: '$CI_PIPELINE_SOURCE == "schedule"'
        - changes:
              - renovate.json
